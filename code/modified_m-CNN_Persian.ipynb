{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"rLlE7uBv1PzH"},"outputs":[],"source":["import os,re,sys,pickle\n","import pandas as pd\n","import tensorflow.keras.backend as K\n","from gensim.corpora import Dictionary\n","from keras.preprocessing.sequence import pad_sequences\n","from hazm import Normalizer, POSTagger, word_tokenize, Lemmatizer, stopwords_list\n","from parsivar import FindStems\n","import numpy as np\n","from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Reshape, Flatten, Dropout, Concatenate\n","from keras.models import Model\n","from keras.layers import BatchNormalization\n","from keras import regularizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LA6YxxjfHAbp"},"outputs":[],"source":["# Set Directories \n","\n","pardir = os.path.dirname(os.getcwd())\n","PATH = os.path.join(pardir,\"data\\\\caption_all_fa\\\\\")\n","os.chdir(PATH)\n","SAVED = os.path.join(pardir,\"data\\\\saved\\\\\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"muDx4aErmeSu"},"outputs":[],"source":["# Prepare for data\n","\n","def load_doc(filename):\n","\tfile = open(file=filename, mode='r', encoding=\"utf-8\")\n","\ttext = file.read()\n","\tfile.close()\n","\treturn text\n","\n","# Map filename to image, text, label for train,evaluation and test\n","def load_clean_descriptions(filename, dataset):\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\ttokens = line.split()\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\tif image_id in dataset:\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\tdesc = ' '.join(image_desc)\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions\n","\n","# Since we are to freeze the VGG16, we have\n","# saved corresponding features for our data\n","# This function is to load those features\n","def load_photo_features(filename, dataset):\n","\tall_features = pickle.load(open(filename, 'rb'))\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features\n","\n","def load_clean_class(filename, dataset):\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\ttokens = line.split()\n","\t\timage_id, image_desc = tokens[0], tokens[1]\n","\t\tif image_desc in dataset:\n","\t\t\tif image_desc not in descriptions:\n","\t\t\t\tdescriptions[image_desc] = list()\n","\t\t\tdescriptions[image_desc].append(image_id)\n","\treturn descriptions\n","\n","# OneHot\n","def load_class_dummy(filename, dataset):\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\ttokens = line.split()\n","\t\timage_id, image_desc = tokens[0], tokens[1]\n","\t\tif image_desc in dataset:\n","\t\t\tif image_desc not in descriptions:\n","\t\t\t\tdescriptions[image_desc] = list()\n","\t\t\timage_id=dummies_dict[image_id]\n","\t\t\tdescriptions[image_desc].append(image_id)\n","\treturn descriptions\n","\n","#Class to one_hot_vector dictionary \n","folder_names = []\n","\n","for entry_name in os.listdir(PATH):\n","    entry_path = os.path.join(PATH, entry_name)\n","    if os.path.isdir(entry_path):\n","        folder_names.append(entry_name)\n","folder_names =sorted(folder_names)\n","\n","dummies = pd.get_dummies(folder_names)\n","dummies_list=dummies.values.tolist()\n","dummies_dict=dict(zip(folder_names,dummies_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11270,"status":"ok","timestamp":1720368352309,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"MQ4LU1PUms26","outputId":"be54845b-c7b8-4369-adc5-c2c6698a29cb"},"outputs":[],"source":["#Image, text, label dictionary for train \n","filename = SAVED+'/train_image.txt'\n","train=[]\n","with open(file=filename, encoding='utf-8', mode='r') as f:\n","    for line in f.read().splitlines():\n","        train=train+[line.split(',')[0][:-4]]\n","\n","train_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","\n","with open(SAVED+\"train_image_features.pkl\",\"rb\") as f:\n","    train_features = pickle.load(f)\n","\n","print('Photos: train=%d' % len(train_features))\n","\n","train_class = load_clean_class(SAVED+'flower_class.txt', train)\n","print('Descriptions: train=%d' % len(train_class))\n","\n","class_dummy=load_class_dummy(SAVED+'flower_class.txt', train)\n","\n","#Image, text, label dictionary for EVAL\n","filename = SAVED+'/val_image.txt'\n","val=[]\n","with open(filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        val=val+[line.split(',')[0][:-4]]\n","\n","val_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', val)\n","print('Descriptions: val=%d' % len(val_descriptions))\n","\n","with open(SAVED+\"eval_image_features.pkl\",\"rb\") as f:\n","    val_features =pickle.load(f)\n","\n","print('Photos: val=%d' % len(val_features))\n","\n","val_class = load_clean_class(SAVED+'flower_class.txt', val)\n","print('Descriptions: val=%d' % len(val_class))\n","val_class_dummy=load_class_dummy(SAVED+'flower_class.txt', val)\n","\n","\n","#Image, text, label dictionary for Test \n","filename = SAVED+'/test_image.txt'\n","test=[]\n","with open(file=filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        test=test+[line.split(',')[0][:-4]]\n","\n","\n","test_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","\n","with open(SAVED+\"test_image_features.pkl\",\"rb\") as f:\n","    test_features =pickle.load(f)\n","\n","print('Photos: test=%d' % len(test_features))\n","\n","test_class = load_clean_class(SAVED+'flower_class.txt', test)\n","print('Descriptions: test=%d' % len(test_class))\n","test_class_dummy=load_class_dummy(SAVED+'flower_class.txt', test)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7nkF23izSeKA"},"outputs":[],"source":["# Load data\n","\n","punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*؟%؛،ًًًََُّ»«_~'''\n","def punc(myStr):\n","  no_punct = \"\"\n","  for char in myStr:\n","    if char not in punctuations:\n","        no_punct = no_punct + char\n","  return no_punct\n","\n","# Load morpheme dictionary for flowers\n","dictionary2=Dictionary.load(SAVED+\"dictionary/flower_dictionary2_fa_lemAndStemWithStopWords\") #choose a dictionary here\n","\n","# Image and text loading and automatic preprocessing function\n","MAX_SEQUENCE_LENGTH=300\n","c=str()\n","\n","normalizer = Normalizer(correct_spacing=True)\n","lemmatizer = Lemmatizer(joined_verb_parts=False)\n","tagger = POSTagger(model=SAVED+'pos_tagger.model')\n","stop_words = stopwords_list()\n","my_stemmer = FindStems()\n","\n","def create_phrase(train_features, train_descriptions, class_dummy,MAX_SEQUENCE_LENGTH):\n","\n","    X_image, X_text, y_class = list(), list(), list()\n","    for key, desc_list in train_descriptions.items():\n","        c=list()\n","        for desc in desc_list:\n","\n","            seq = punc(desc)\n","            normalized_text = normalizer.normalize(seq)\n","            tokens = word_tokenize(normalized_text)\n","\n","            newTokens = []\n","\n","            tokenAndTag = tagger.tag(tokens)\n","            newTokenAndTag = []\n","            for TAT in tokenAndTag:\n","              newTokenAndTag.append([TAT[0], TAT[1].replace('PRON', 'PRO').replace('ADJ', 'AJ').replace('VERB', 'V').split(',')[0]])\n","            tokenAndTag = newTokenAndTag\n","\n","            j = 0\n","            for token in tokens :\n","              for lemToken in lemmatizer.lemmatize(word=token, pos=tokenAndTag[j][1]).split('#') :\n","                  if lemToken != '' :#if lemToken not in stop_words and lemToken != ''  : #using StopWorks\n","                    newTokens.append(lemToken)\n","              for stemToken in my_stemmer.convert_to_stem(token).split('&') :\n","                  if  stemToken != ''  :#if stemToken not in stop_words and stemToken != ''  :  #using StopWorks\n","                    newTokens.append(stemToken)\n","              j += 1\n","            tokens = newTokens\n","            tokens = [token for token in tokens if token != '']\n","\n","            b = tagger.tag(tokens)\n","\n","            aaa=[]\n","            for y in b:\n","                aaa.append(y[0])\n","\n","            c+=aaa\n","        d=dictionary2.doc2idx(c)\n","        X_text.append(np.array(d))\n","        if key in class_dummy:\n","            y_class.append(np.array(class_dummy[key][0]))\n","        X_image.append(train_features[key][0])\n","\n","    X_text=pad_sequences(X_text, maxlen=MAX_SEQUENCE_LENGTH)\n","    return np.array(X_text),np.array(X_image),np.array(y_class)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Q-qYZLnwm2Tr"},"outputs":[],"source":["#Import images and text into training, validation, and test sets\n","X_text, X_image, y_class =create_phrase(train_features, train_descriptions, class_dummy,MAX_SEQUENCE_LENGTH)\n","X_text_val, X_image_val, y_class_val =create_phrase(val_features, val_descriptions, val_class_dummy,MAX_SEQUENCE_LENGTH)\n","X_text_test, X_image_test, y_class_test =create_phrase(test_features, test_descriptions, test_class_dummy,MAX_SEQUENCE_LENGTH)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1326,"status":"ok","timestamp":1720368484240,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"kFrtLZ0sm8pc","outputId":"82791895-40fe-4aca-d525-584fd48b8a2a"},"outputs":[],"source":["# Load pre-trained embedding matrix\n","EMBEDDING_DIM=300  #skipgram Dim\n","word_index=dictionary2.token2id \n","\n","embedding_path = SAVED + 'skipgram/skigram1_fa_new_300_lemStemWithStopWords.txt' #choose a skipgram here\n","\n","embeddings_index = dict()\n","f = open(file = embedding_path, encoding= \"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # words not found in embedding index will be all-zeros.\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OgaVS570nEaQ"},"outputs":[],"source":["# Model definition\n","\n","def Modified_m_CNN(DROP_OUT,DROP_OUT2,LAMBDA):\n","\tinputs1 = Input(shape=(4096,))\n","\tx= Reshape((16,1,256))(inputs1)\n","\tx=BatchNormalization()(x)\n","\tconv_x=Conv2D(256, kernel_size=(14,1), padding='valid', kernel_initializer='he_normal', activation='relu')(x)\n","\n","\tconv_x = Dropout(DROP_OUT2)(conv_x)\n","\tMax_x = MaxPool2D(pool_size=(2,1))(conv_x)\n","\tinputs2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n","\ty = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True) (inputs2)\n","\treshape = Reshape((MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,1))(y)\n","\tconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBEDDING_DIM), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape)\n","\tconv_0=BatchNormalization()(conv_0)\n","\tconv_0 = Dropout(DROP_OUT2)(conv_0)\n","\tconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBEDDING_DIM), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape)\n","\tconv_1 =BatchNormalization()(conv_1 )\n","\tconv_1 = Dropout(DROP_OUT2)(conv_1)\n","\tconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBEDDING_DIM), padding='valid', kernel_initializer='he_normal', activation='relu')(reshape)\n","\tconv_2 =BatchNormalization()(conv_2 )\n","\tconv_2 = Dropout(DROP_OUT2)(conv_2)\n","\tmaxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n","\tmaxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n","\tmaxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n","\tconcat1 = Concatenate(axis=1)([maxpool_0,Max_x])\n","\tconcat2 = Concatenate(axis=1)([maxpool_1,Max_x])\n","\tconcat3 = Concatenate(axis=1)([maxpool_2,Max_x])\n","\tconcat4 = Concatenate(axis=1)([concat1, concat2 ,concat3 ])\n","\ta=Conv2D(512, kernel_size=(5,1), padding='valid', kernel_initializer='he_normal', activation='relu')(concat4)\n","\ta=BatchNormalization()(a)\n","\ta=Dropout(DROP_OUT2)(a)\n","\ta=MaxPool2D(pool_size=(2,1))(a)\n","\ta = Flatten()(a)\n","\tz = Dropout(DROP_OUT)(a)\n","\toutput = Dense(units=102, activation='softmax',kernel_regularizer=regularizers.l2(LAMBDA), kernel_initializer='he_normal')(z)\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=output)\n","\t#print(model.summary())\n","\treturn model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11326803,"status":"ok","timestamp":1720379811038,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"d1CrQLu3nKs8","outputId":"f8bc779e-1f91-4eba-9595-f9925b9fc22e"},"outputs":[],"source":["BATCH_SIZE = 128\n","EPOCHS = 40\n","LAMBDA=0.05\n","DROP_OUT=0.2\n","DROP_OUT2=0.4\n","filter_sizes = [2,3,4]\n","num_filters = 256\n","\n","model=Modified_m_CNN(DROP_OUT,DROP_OUT2,LAMBDA)\n","\n","# Model detailed settings\n","model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n","\n","# model fit\n","history = model.fit([X_image,X_text], y_class, epochs=EPOCHS,\n","     batch_size=BATCH_SIZE,validation_data=([X_image_val,X_text_val],y_class_val), shuffle=True)\n","\n","# model evaluation\n","score = model.evaluate([X_image_test,X_text_test], y_class_test, verbose=1)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n","#score[1]\n","\n","#accuracy: 92.96% skigram1_fa_new_300_lemStem.txt\n","#accuracy: 92.65% LAMBDA=0.1 skigram1_fa_new_300_lemStem.txt\n","\n","#accuracy: 92.41% skigram1_fa_new_100_lemStem.txt\n","#accuracy: 93.02% skigram1_fa_new_200_lemStem.txt\n","\n","#accuracy: 92.71% skigram1_fa_new_200_lemOnlyWithStopWords.txt\n","#accuracy: 92.90% EPOCHS = 50 skigram1_fa_new_200_lemOnlyWithStopWords.txt\n","\n","#accuracy: 93.57% skigram1_fa_new_250_lemOnlyWithStopWords.txt\n","#accuracy: 93.14% EPOCHS = 50 skigram1_fa_new_250_lemOnlyWithStopWords.txt\n","\n","#accuracy: 93.82% skigram1_fa_new_300_lemOnlyWithStopWords.txt\n","#accuracy: 93.02% skigram1_fa_new_100_lemOnlyWithStopWords.txt\n","\n","#accuracy: 92.71% skigram1_fa_new_200_lemStemWithStopWords\n","#accuracy: 93.08% skigram1_fa_new_300_lemStemWithStopWords\n","\n","#accuracy: 94.24% skigram1_fa_new_250_lemStemWithStopWords\n","#accuracy: 93.20% skigram1_fa_new_250_lemStemWithStopWords\n","#accuracy: 93.39% skigram1_fa_new_250_lemStemWithStopWords\n","#accuracy: 93.08%\n","#accuracy: 93.33%\n","#accuracy: 93.82%\n","#accuracy: 92.84%\n","#accuracy: 93.26%\n","#accuracy: 93.69%\n","\n","#accuracy: 93.69% EPOCHS = 50 skigram1_fa_new_250_lemStemWithStopWords\n","#accuracy: 92.77% skigram1_fa_new_150_lemStemWithStopWords\n","\n","#accuracy: 94.00% skigram1_fa_new_300_lemStemWithStopWords\n","#accuracy: 93.33% skigram1_fa_new_300_lemStemWithStopWords\n","#accuracy: 94.06% skigram1_fa_new_300_lemStemWithStopWords\n","#accuracy: 92.96% skigram1_fa_new_300_lemStemWithStopWords\n","#accuracy: 93.45% skigram1_fa_new_300_lemStemWithStopWords\n","#accuracy: 93.26%\n","#accuracy: 93.14%\n","#accuracy: 94.67%\n","#accuracy: 93.94%\n","#accuracy: 93.26%\n","\n","#accuracy: 93.20% BATCH_SIZE = 64 skigram1_fa_new_300_lemStemWithStopWords\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPEu5S1D4QUWjiK3p1SeqpS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
