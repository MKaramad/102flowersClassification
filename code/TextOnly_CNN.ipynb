{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2734,"status":"ok","timestamp":1721139598071,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"CXOP_6ongkzl"},"outputs":[],"source":["import os\n","import pandas as pd\n","import tensorflow.keras.backend as K\n","from gensim.corpora import Dictionary\n","from keras.preprocessing.sequence import pad_sequences\n","from hazm import Normalizer, POSTagger, word_tokenize, Lemmatizer, stopwords_list\n","from parsivar import FindStems\n","import numpy as np\n","from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D,Reshape, Flatten, Dropout, Concatenate\n","from keras.models import Model\n","from keras.layers import BatchNormalization \n","from keras import regularizers\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Bidirectional, Attention,  Embedding, LSTM, Dense"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1721139601544,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"7FTtouNSgnYl"},"outputs":[],"source":["# Set Directories \n","\n","pardir = os.path.dirname(os.getcwd())\n","PATH = os.path.join(pardir,\"data\\\\caption_all_fa\\\\\")\n","os.chdir(PATH)\n","SAVED = os.path.join(pardir,\"data\\\\saved\\\\\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721139603451,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"46yq_155gpmS"},"outputs":[],"source":["# Prepare for data\n","\n","def load_doc(filename):\n","    file = open(file=filename, mode='r', encoding=\"utf-8\")\n","    text = file.read()\n","    file.close()\n","    return text\n","\n","# Map filename to image,text,label for train,evaluation and test\n","def load_clean_descriptions(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        if image_id in dataset:\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            desc = ' '.join(image_desc)\n","            descriptions[image_id].append(desc)\n","    return descriptions\n","\n","def load_clean_class(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","# OneHot\n","def load_class_dummy(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            image_id=dummies_dict[image_id]\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","### Class to one_hot_vector dictionary ###\n","folder_names = []\n","\n","for entry_name in os.listdir(PATH):\n","    entry_path = os.path.join(PATH, entry_name)\n","    if os.path.isdir(entry_path):\n","        folder_names.append(entry_name)\n","folder_names =sorted(folder_names)\n","\n","dummies = pd.get_dummies(folder_names)\n","dummies_list=dummies.values.tolist()\n","dummies_dict=dict(zip(folder_names,dummies_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9356,"status":"ok","timestamp":1721139616526,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"egoAXsijgyfB","outputId":"73f96de1-41e7-44c7-ebc9-cccf4d657d6d"},"outputs":[],"source":["### Text and label dictionary for train ###\n","filename = SAVED+'/train_image.txt'\n","train=[]\n","with open(file=filename, encoding='utf-8', mode='r') as f:\n","    for line in f.read().splitlines():\n","        train=train+[line.split(',')[0][:-4]]\n","\n","train_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","\n","train_class = load_clean_class(SAVED+'flower_class.txt', train)\n","print('Descriptions: train=%d' % len(train_class))\n","\n","class_dummy=load_class_dummy(SAVED+'flower_class.txt', train)\n","\n","### Text and label dictionary for EVAL ###\n","filename = SAVED+'/val_image.txt'\n","val=[]\n","with open(filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        val=val+[line.split(',')[0][:-4]]\n","\n","val_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', val)\n","print('Descriptions: val=%d' % len(val_descriptions))\n","\n","val_class = load_clean_class(SAVED+'flower_class.txt', val)\n","print('Descriptions: val=%d' % len(val_class))\n","val_class_dummy=load_class_dummy(SAVED+'flower_class.txt', val)\n","\n","\n","### Text and label dictionary for Test ###\n","filename = SAVED+'/test_image.txt'\n","test=[]\n","with open(file=filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        test=test+[line.split(',')[0][:-4]]\n","\n","test_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","\n","test_class = load_clean_class(SAVED+'flower_class.txt', test)\n","print('Descriptions: test=%d' % len(test_class))\n","test_class_dummy=load_class_dummy(SAVED+'flower_class.txt', test)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6932,"status":"ok","timestamp":1721139635361,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"i0QAqH3-g70J"},"outputs":[],"source":["# Load data\n","\n","punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*؟%؛،ًًًََُّ»«_~'''\n","def punc(myStr):\n","  no_punct = \"\"\n","  for char in myStr:\n","    if char not in punctuations:\n","        no_punct = no_punct + char\n","  return no_punct\n","\n","# Load morpheme dictionary for flowers\n","dictionary2=Dictionary.load(SAVED+\"dictionary/flower_dictionary2_fa_lemAndStemWithStopWords\")\n","\n","# Text loading and automatic preprocessing function\n","MAX_SEQUENCE_LENGTH=300\n","c=str()\n","\n","normalizer = Normalizer(correct_spacing=True)\n","lemmatizer = Lemmatizer(joined_verb_parts=False)\n","tagger = POSTagger(model=SAVED+'pos_tagger.model')\n","stop_words = stopwords_list()\n","my_stemmer = FindStems()\n","\n","def create_phrase(train_descriptions, class_dummy, MAX_SEQUENCE_LENGTH):\n","\n","    X_train, y_class = list(), list()\n","\n","    for key, desc_list in train_descriptions.items():\n","        c=list()\n","        for desc in desc_list:\n","\n","            seq = punc(desc)\n","            normalized_text = normalizer.normalize(seq)\n","            tokens = word_tokenize(normalized_text)\n","\n","            newTokens = []\n","\n","            tokenAndTag = tagger.tag(tokens)\n","            newTokenAndTag = []\n","            for TAT in tokenAndTag:\n","              newTokenAndTag.append([TAT[0], TAT[1].replace('PRON', 'PRO').replace('ADJ', 'AJ').replace('VERB', 'V').split(',')[0]])\n","            tokenAndTag = newTokenAndTag\n","\n","            j = 0\n","            for token in tokens :\n","              for lemToken in lemmatizer.lemmatize(word=token, pos=tokenAndTag[j][1]).split('#') :\n","                  if lemToken != '' :#if lemToken not in stop_words and lemToken != ''  :\n","                    newTokens.append(lemToken)\n","              for stemToken in my_stemmer.convert_to_stem(token).split('&') :\n","                  if  stemToken != ''  :#if stemToken not in stop_words and stemToken != ''  :\n","                    newTokens.append(stemToken)\n","              j += 1\n","            tokens = newTokens\n","            tokens = [token for token in tokens if token != '']\n","\n","            b = tagger.tag(tokens)\n","\n","            aaa=[]\n","            for y in b:\n","                aaa.append(y[0])\n","\n","            c+=aaa\n","        d=dictionary2.doc2idx(c)\n","        X_train.append(np.array(d))\n","        if key in class_dummy:\n","            y_class.append(np.array(class_dummy[key][0]))\n","\n","    X_train=pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","    return np.array(X_train),np.array(y_class)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":120980,"status":"ok","timestamp":1721139758271,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"I3PNICWbhDns"},"outputs":[],"source":["# Import text into training, validation, and test sets\n","X_train, y_train = create_phrase(train_descriptions, class_dummy,MAX_SEQUENCE_LENGTH)\n","X_val, y_val = create_phrase(val_descriptions, val_class_dummy,MAX_SEQUENCE_LENGTH)\n","X_test, y_test = create_phrase(test_descriptions, test_class_dummy,MAX_SEQUENCE_LENGTH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2595,"status":"ok","timestamp":1721139763327,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"YwCZxC4uhG_C","outputId":"dd33135b-a49e-4b9f-fbae-a7075b282006"},"outputs":[],"source":["# Load pre-trained embedding matrix\n","EMBEDDING_DIM=300\n","word_index=dictionary2.token2id\n","\n","embedding_path = SAVED + 'skipgram/skigram1_fa_new_300_lemStemWithStopWords.txt'\n","\n","embeddings_index = dict()\n","f = open(file = embedding_path, encoding= \"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1721139769436,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"3eiUNBz2hOIY"},"outputs":[],"source":["### Text CNN(Kim_2014) ###\n","\n","def text_CNN(DROP_OUT, DROP_OUT2, LAMBDA, DENSE_NUM):\n","    embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n","\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","\n","    reshape = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(embedded_sequences)\n","\n","    conv_layers = []\n","\n","    for filter_size in filter_sizes:\n","        conv = Conv2D(num_filters, kernel_size=(filter_size, EMBEDDING_DIM), activation='relu')(reshape)\n","        maxpool = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_size + 1, 1))(conv)\n","        conv_layers.append(maxpool)\n","\n","    concat = Concatenate(axis=1)(conv_layers)\n","    flatten = Flatten()(concat)\n","    dropout = Dropout(DROP_OUT)(flatten)\n","\n","    #dense = Dense(DENSE_NUM, activation='relu')(dropout)\n","    #dropout = Dropout(DROP_OUT2)(dense)\n","\n","    output = Dense(units=102, activation='softmax', kernel_regularizer=regularizers.l2(LAMBDA), kernel_initializer='he_normal')(dropout)\n","\n","    model = Model(inputs=sequence_input, outputs=output)\n","\n","    return model\n","\n","\n","# Training model\n","\n","BATCH_SIZE = 128\n","EPOCHS = 80\n","LAMBDA=0.065#0.05\n","DROP_OUT=0.065#0.2\n","DROP_OUT2=0.125#0.4\n","filter_sizes = [2, 3, 4]\n","num_filters = 256\n","\n","DENSE_NUM=128\n","\n","model = text_CNN(DROP_OUT, DROP_OUT2, LAMBDA, DENSE_NUM)\n","\n","rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True, momentum=0.7)\n","\n","\n","model.compile(loss='categorical_crossentropy', optimizer=rmsOpt, metrics=['accuracy'])#adam\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val), callbacks=[early_stopping],validation_batch_size=BATCH_SIZE*2)\n","\n","# Evaluation on test data\n","loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n","print(f'Test Accuracy: {accuracy}')\n","\n","# + denseLayer 77.01% batch_size = 128 EPOCHS = 20 LAMBDA=0.05#0.05 DROP_OUT=0.05#0.2 DROP_OUT2=0.1#0.4 filter_sizes = [2, 3, 4] num_filters = 256 DENSE_NUM=128  optimizer='Adam'\n","#Test Accuracy: 0.7477036118507385 EPOCHS = 50 optimizer='RMSProp'\n","#Test Accuracy: 0.7428046464920044 rmsOpt = RMSprop(learning_rate = 0.0025, weight_decay=0.0001)\n","#Test Accuracy: 0.7813839316368103 LAMBDA=0.06 DROP_OUT=0.06 DROP_OUT2=0.12 EPOCHS = 60 rmsOpt = RMSprop(learning_rate = 0.0025, weight_decay=0.000115, use_ema=True) validation_batch_size=BATCH_SIZE*2\n","#Test Accuracy: 0.7556644082069397 LAMBDA=0.065 DROP_OUT=0.065 DROP_OUT2=0.125 EPOCHS = 80 rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True) validation_batch_size=BATCH_SIZE*2\n","#Test Accuracy: 0.7556644082069397 LAMBDA=0.065 DROP_OUT=0.065 DROP_OUT2=0.125 EPOCHS = 80 rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True) validation_batch_size=BATCH_SIZE*4\n","#Test Accuracy: 0.7752602696418762 LAMBDA=0.065 DROP_OUT=0.065 DROP_OUT2=0.125 EPOCHS = 80 rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True) validation_batch_size=BATCH_SIZE*4\n","#Test Accuracy: 0.7905694842338562 rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True, momentum=0.7) validation_batch_size=BATCH_SIZE*4\n","\n","# + denseLayer Test Accuracy: 0.793631374835968 rmsOpt = RMSprop(learning_rate = 0.0027, weight_decay=0.000117, use_ema=True, centered=True, momentum=0.7) validation_batch_size=BATCH_SIZE*4\n","# + denseLayer rmsOpt = RMSprop(learning_rate = 0.002, weight_decay=0.0001, use_ema=True) validation_batch_size=BATCH_SIZE*2 batch_size = 128 EPOCHS = 80 LAMBDA=0.05#0.05 DROP_OUT=0.05#0.2 DROP_OUT2=0.1#0.4 filter_sizes = [2, 3, 4] num_filters = 256 DENSE_NUM=128"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFT2AzS5/CVz3CktjX0DTN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
