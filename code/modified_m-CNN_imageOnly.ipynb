{"cells":[{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":6374,"status":"ok","timestamp":1721304430813,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"Y8JxlJ4IirCw"},"outputs":[],"source":["import os, pickle\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Input, Conv2D, MaxPool2D, Reshape, Flatten, Dense, Dropout, BatchNormalization\n","from keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721304432586,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"LA6YxxjfHAbp"},"outputs":[],"source":["# Set Directories \n","\n","pardir = os.path.dirname(os.getcwd())\n","PATH = os.path.join(pardir,\"data\\\\caption_all_fa\\\\\")\n","os.chdir(PATH)\n","SAVED = os.path.join(pardir,\"data\\\\saved\\\\\")"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":621,"status":"ok","timestamp":1721304435440,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"muDx4aErmeSu"},"outputs":[],"source":["# Load data functions\n","\n","def load_doc(filename):\n","    with open(filename, 'r', encoding=\"utf-8\") as file:\n","        text = file.read()\n","    return text\n","\n","def load_clean_class(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","def load_class_dummy(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            image_id = dummies_dict[image_id]\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","# Class to one_hot_vector dictionary\n","folder_names = sorted([entry_name for entry_name in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, entry_name))])\n","dummies = pd.get_dummies(folder_names)\n","dummies_list = dummies.values.tolist()\n","dummies_dict = dict(zip(folder_names, dummies_list))\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3253,"status":"ok","timestamp":1721305244025,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"cHq1pc8Cmg5W"},"outputs":[],"source":["# Load train data\n","filename = SAVED + '/train_image.txt'\n","train = [line.split(',')[0][:-4] for line in open(filename, encoding='utf-8').read().splitlines()]\n","\n","with open(SAVED + \"train_image_features_ENetB2.pkl\", \"rb\") as f: #choose image features\n","    train_features = pickle.load(f)\n","\n","train_class = load_clean_class(SAVED + 'flower_class.txt', train)\n","class_dummy = load_class_dummy(SAVED + 'flower_class.txt', train)\n","\n","# Load val data\n","filename = SAVED + '/val_image.txt'\n","val = [line.split(',')[0][:-4] for line in open(filename, encoding='utf-8').read().splitlines()]\n","\n","with open(SAVED + \"eval_image_features_ENetB2.pkl\", \"rb\") as f: #choose image features\n","    val_features = pickle.load(f)\n","\n","val_class = load_clean_class(SAVED + 'flower_class.txt', val)\n","val_class_dummy = load_class_dummy(SAVED + 'flower_class.txt', val)\n","\n","# Load test data\n","filename = SAVED + '/test_image.txt'\n","test = [line.split(',')[0][:-4] for line in open(filename, encoding='utf-8').read().splitlines()]\n","\n","with open(SAVED + \"test_image_features_ENetB2.pkl\", \"rb\") as f: #choose image features\n","    test_features = pickle.load(f)\n","\n","test_class = load_clean_class(SAVED + 'flower_class.txt', test)\n","test_class_dummy = load_class_dummy(SAVED + 'flower_class.txt', test)\n","\n","# Prepare data for training\n","def prepare_data(features, class_dummy):\n","    X_image, y_class = [], []\n","    for key in features.keys():\n","        X_image.append(features[key][0])\n","        key = key.replace(\".jpg\", \"\")\n","        if key in class_dummy:\n","            y_class.append(np.array(class_dummy[key][0]))\n","    return np.array(X_image), np.array(y_class)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":603,"status":"ok","timestamp":1721305365999,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"7nkF23izSeKA"},"outputs":[],"source":["X_image, y_class = prepare_data(train_features, class_dummy)\n","X_image_val, y_class_val = prepare_data(val_features, val_class_dummy)\n","X_image_test, y_class_test = prepare_data(test_features, test_class_dummy)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":614,"status":"ok","timestamp":1721305368826,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"kFrtLZ0sm8pc"},"outputs":[],"source":["# Model definition\n","def Modified_m_CNN(DROP_OUT, DROP_OUT2, LAMBDA):\n","    inputs = Input(shape=(4096,))\n","    x = Reshape((16, 1, 256))(inputs)\n","    x = BatchNormalization()(x)\n","    conv_x = Conv2D(256, kernel_size=(14, 1), padding='valid', kernel_initializer='he_normal', activation='relu')(x)\n","    conv_x = Dropout(DROP_OUT2)(conv_x)\n","    max_x = MaxPool2D(pool_size=(2, 1))(conv_x)\n","    x = Flatten()(max_x)\n","    x = Dropout(DROP_OUT)(x)\n","    output = Dense(units=102, activation='softmax', kernel_regularizer='l2')(x)\n","    model = Model(inputs=inputs, outputs=output)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":825920,"status":"ok","timestamp":1721308324584,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"d1CrQLu3nKs8","outputId":"be81a930-a993-46af-b2e3-a728ff67e01a"},"outputs":[],"source":["BATCH_SIZE = 128\n","EPOCHS = 40\n","LAMBDA = 0.05\n","DROP_OUT = 0.2\n","DROP_OUT2 = 0.4\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","model = Modified_m_CNN(DROP_OUT, DROP_OUT2, LAMBDA)\n","\n","# Model detailed settings\n","model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n","\n","# Model fit\n","history = model.fit(X_image, y_class, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_image_val, y_class_val), shuffle=True, callbacks=[early_stopping])\n","\n","# Model evaluation\n","score = model.evaluate(X_image_test, y_class_test, verbose=1)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO+fjlcajIsIEg6Fphru4FJ","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
