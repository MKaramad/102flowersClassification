{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2734,"status":"ok","timestamp":1721139598071,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"CXOP_6ongkzl"},"outputs":[],"source":["import os\n","import pandas as pd\n","import tensorflow.keras.backend as K\n","from gensim.corpora import Dictionary\n","from keras.preprocessing.sequence import pad_sequences\n","from hazm import Normalizer, POSTagger, word_tokenize, Lemmatizer, stopwords_list\n","from parsivar import FindStems\n","import numpy as np\n","from keras.layers import Input, Dense, Embedding, Dropout\n","from keras.models import Model\n","from keras.layers import BatchNormalization \n","from keras import regularizers\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Embedding, LSTM, Dense"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1721139601544,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"7FTtouNSgnYl"},"outputs":[],"source":["# Set Directories \n","\n","pardir = os.path.dirname(os.getcwd())\n","PATH = os.path.join(pardir,\"data\\\\caption_all_fa\\\\\")\n","os.chdir(PATH)\n","SAVED = os.path.join(pardir,\"data\\\\saved\\\\\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721139603451,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"46yq_155gpmS"},"outputs":[],"source":["# Prepare for data\n","\n","def load_doc(filename):\n","    file = open(file=filename, mode='r', encoding=\"utf-8\")\n","    text = file.read()\n","    file.close()\n","    return text\n","\n","# Map filename to image,text,label for train,evaluation and test\n","def load_clean_descriptions(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        if image_id in dataset:\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            desc = ' '.join(image_desc)\n","            descriptions[image_id].append(desc)\n","    return descriptions\n","\n","def load_clean_class(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","# OneHot\n","def load_class_dummy(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            image_id=dummies_dict[image_id]\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","### Class to one_hot_vector dictionary ###\n","folder_names = []\n","\n","for entry_name in os.listdir(PATH):\n","    entry_path = os.path.join(PATH, entry_name)\n","    if os.path.isdir(entry_path):\n","        folder_names.append(entry_name)\n","folder_names =sorted(folder_names)\n","\n","dummies = pd.get_dummies(folder_names)\n","dummies_list=dummies.values.tolist()\n","dummies_dict=dict(zip(folder_names,dummies_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9356,"status":"ok","timestamp":1721139616526,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"egoAXsijgyfB","outputId":"73f96de1-41e7-44c7-ebc9-cccf4d657d6d"},"outputs":[],"source":["### Text and label dictionary for train ###\n","filename = SAVED+'/train_image.txt'\n","train=[]\n","with open(file=filename, encoding='utf-8', mode='r') as f:\n","    for line in f.read().splitlines():\n","        train=train+[line.split(',')[0][:-4]]\n","\n","train_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","\n","train_class = load_clean_class(SAVED+'flower_class.txt', train)\n","print('Descriptions: train=%d' % len(train_class))\n","\n","class_dummy=load_class_dummy(SAVED+'flower_class.txt', train)\n","\n","### Text and label dictionary for EVAL ###\n","filename = SAVED+'/val_image.txt'\n","val=[]\n","with open(filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        val=val+[line.split(',')[0][:-4]]\n","\n","val_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', val)\n","print('Descriptions: val=%d' % len(val_descriptions))\n","\n","val_class = load_clean_class(SAVED+'flower_class.txt', val)\n","print('Descriptions: val=%d' % len(val_class))\n","val_class_dummy=load_class_dummy(SAVED+'flower_class.txt', val)\n","\n","\n","### Text and label dictionary for Test ###\n","filename = SAVED+'/test_image.txt'\n","test=[]\n","with open(file=filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        test=test+[line.split(',')[0][:-4]]\n","\n","test_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","\n","test_class = load_clean_class(SAVED+'flower_class.txt', test)\n","print('Descriptions: test=%d' % len(test_class))\n","test_class_dummy=load_class_dummy(SAVED+'flower_class.txt', test)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6932,"status":"ok","timestamp":1721139635361,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"i0QAqH3-g70J"},"outputs":[],"source":["# Load data\n","\n","punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*؟%؛،ًًًََُّ»«_~'''\n","def punc(myStr):\n","  no_punct = \"\"\n","  for char in myStr:\n","    if char not in punctuations:\n","        no_punct = no_punct + char\n","  return no_punct\n","\n","# Load morpheme dictionary for flowers\n","dictionary2=Dictionary.load(SAVED+\"dictionary/flower_dictionary2_fa_lemAndStemWithStopWords\")\n","\n","# Text loading and automatic preprocessing function\n","MAX_SEQUENCE_LENGTH=300\n","c=str()\n","\n","normalizer = Normalizer(correct_spacing=True)\n","lemmatizer = Lemmatizer(joined_verb_parts=False)\n","tagger = POSTagger(model=SAVED+'pos_tagger.model')\n","stop_words = stopwords_list()\n","my_stemmer = FindStems()\n","\n","def create_phrase(train_descriptions, class_dummy, MAX_SEQUENCE_LENGTH):\n","\n","    X_train, y_class = list(), list()\n","\n","    for key, desc_list in train_descriptions.items():\n","        c=list()\n","        for desc in desc_list:\n","\n","            seq = punc(desc)\n","            normalized_text = normalizer.normalize(seq)\n","            tokens = word_tokenize(normalized_text)\n","\n","            newTokens = []\n","\n","            tokenAndTag = tagger.tag(tokens)\n","            newTokenAndTag = []\n","            for TAT in tokenAndTag:\n","              newTokenAndTag.append([TAT[0], TAT[1].replace('PRON', 'PRO').replace('ADJ', 'AJ').replace('VERB', 'V').split(',')[0]])\n","            tokenAndTag = newTokenAndTag\n","\n","            j = 0\n","            for token in tokens :\n","              for lemToken in lemmatizer.lemmatize(word=token, pos=tokenAndTag[j][1]).split('#') :\n","                  if lemToken != '' :#if lemToken not in stop_words and lemToken != ''  :\n","                    newTokens.append(lemToken)\n","              for stemToken in my_stemmer.convert_to_stem(token).split('&') :\n","                  if  stemToken != ''  :#if stemToken not in stop_words and stemToken != ''  :\n","                    newTokens.append(stemToken)\n","              j += 1\n","            tokens = newTokens\n","            tokens = [token for token in tokens if token != '']\n","\n","            b = tagger.tag(tokens)\n","\n","            aaa=[]\n","            for y in b:\n","                aaa.append(y[0])\n","\n","            c+=aaa\n","        d=dictionary2.doc2idx(c)\n","        X_train.append(np.array(d))\n","        if key in class_dummy:\n","            y_class.append(np.array(class_dummy[key][0]))\n","\n","    X_train=pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","    return np.array(X_train),np.array(y_class)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":120980,"status":"ok","timestamp":1721139758271,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"I3PNICWbhDns"},"outputs":[],"source":["# Import text into training, validation, and test sets\n","X_train, y_train = create_phrase(train_descriptions, class_dummy,MAX_SEQUENCE_LENGTH)\n","X_val, y_val = create_phrase(val_descriptions, val_class_dummy,MAX_SEQUENCE_LENGTH)\n","X_test, y_test = create_phrase(test_descriptions, test_class_dummy,MAX_SEQUENCE_LENGTH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2595,"status":"ok","timestamp":1721139763327,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"YwCZxC4uhG_C","outputId":"dd33135b-a49e-4b9f-fbae-a7075b282006"},"outputs":[],"source":["# Load pre-trained embedding matrix\n","EMBEDDING_DIM=300\n","word_index=dictionary2.token2id\n","\n","embedding_path = SAVED + 'skipgram/skigram1_fa_new_300_lemStemWithStopWords.txt'\n","\n","embeddings_index = dict()\n","f = open(file = embedding_path, encoding= \"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#LSTM\n","\n","def text_LSTM(DROP_OUT1, DROP_OUT2, DENSE_NUM, LAMBDA1, LAMBDA2):\n","    embedding_layer = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n","\n","    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')\n","    embedded_sequences = embedding_layer(sequence_input)\n","    \n","    # LSTM layer\n","    lstm = LSTM(128, return_sequences=True)(embedded_sequences)\n","    lstm = LSTM(64)(lstm)\n","    dropout = Dropout(DROP_OUT1)(lstm)\n","\n","    dense = Dense(DENSE_NUM, activation='relu')(dropout)\n","    \n","    dense = BatchNormalization()(dense)\n","\n","    dropout = Dropout(DROP_OUT2)(dense)\n","\n","    output = Dense(units=102, activation='softmax', kernel_regularizer=regularizers.L1L2(l1=LAMBDA1, l2=LAMBDA2), kernel_initializer='he_normal')(dropout)\n","\n","    model = Model(sequence_input, output)\n","\n","    return model\n","\n","BATCH_SIZE = 128\n","EPOCHS = 80\n","DROP_OUT1 = 0.001\n","DROP_OUT2 = 0.005\n","LAMBDA1 = 0.01\n","LAMBDA2 = 0.05\n","DENSE_NUM = 128\n","\n","model = text_LSTM(DROP_OUT1, DROP_OUT2, DENSE_NUM, LAMBDA1, LAMBDA2)\n","\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","initial_learning_rate = 0.0025\n","'''lr_schedule = ExponentialDecay(\n","    initial_learning_rate,\n","    decay_steps=82,\n","    decay_rate=0.92,\n","    staircase=True)\n","rmsOpt = RMSprop(learning_rate = lr_schedule)\n","'''\n","'''\n","initial_learning_rate = 0.0025\n","boundaries = [410, 820, 1230, 1640, 2050, 2460] #[5, 10, 15, 20, 25, 30 ]\n","values = [0.002, 0.0015, 0.001, 0.0008, 0.0005, 0.0003, 0.0002]\n","learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n","'''\n","\n","#rmsOpt = RMSprop(learning_rate = learning_rate_fn)\n","\n","rmsOpt = RMSprop(learning_rate = 0.002, weight_decay=0.00025, use_ema=True)\n","\n","model.compile(loss='categorical_crossentropy', optimizer=rmsOpt, metrics=['accuracy'])#optimizer=rmsOpt\n","\n","print(BATCH_SIZE,\n","    EPOCHS,\n","    DROP_OUT1,\n","    DROP_OUT2,\n","    LAMBDA1,\n","    LAMBDA2,\n","    DENSE_NUM)\n","\n","history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[early_stopping], validation_data=(X_val, y_val), shuffle=True, validation_batch_size=BATCH_SIZE*4)\n","\n","score = model.evaluate(X_test, y_test, verbose=1)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n","\n","#compile_metrics: 60.20%\n","#compile_metrics: 60.62%\n","#compile_metrics: 62.52% BATCH_SIZE = 128 EPOCHS = 40 DROP_OUT1 = 0.00 DROP_OUT2 = 0.00 LAMBDA1 = 0.01 LAMBDA2 = 0.05 DENSE_NUM = 128 learning_rate = 0.002\n","#compile_metrics: 65.16% BATCH_SIZE = 64 EPOCHS = 60 DROP_OUT1 = 0.001 DROP_OUT2 = 0.005\n","#compile_metrics: 60.07% BATCH_SIZE = 64 EPOCHS = 80 DROP_OUT1 = 0.01 DROP_OUT2 = 0.05 LAMBDA1 = 0.02 LAMBDA2 = 0.1\n","#compile_metrics: 52.05% BATCH_SIZE = 64 EPOCHS = 120 DROP_OUT1 = 0.05 DROP_OUT2 = 0.1 LAMBDA1 = 0.05 LAMBDA2 = 0.1\n","\n","#compile_metrics: 65.16% BATCH_SIZE = 32 EPOCHS = 80 DROP_OUT1 = 0.001 DROP_OUT2 = 0.005\n","#compile_metrics: 66.93% BATCH_SIZE = 64 EPOCHS = 80 DROP_OUT1 = 0.001 DROP_OUT2 = 0.005 LAMBDA1 = 0.01 LAMBDA2 = 0.05\n","\n","#compile_metrics: 67.18% BATCH_SIZE = 64 EPOCHS = 80 DROP_OUT1 = 0.001 DROP_OUT2 = 0.005 LAMBDA1 = 0.01 LAMBDA2 = 0.05  validation_batch_size=BATCH_SIZE*4 \n","#rmsOpt = RMSprop(learning_rate = 0.003, weight_decay=0.000155) \n","\n","#compile_metrics: 67.18% BATCH_SIZE = 64 EPOCHS = 80 DROP_OUT1 = 0.0015 DROP_OUT2 = 0.008 LAMBDA1 = 0.015 LAMBDA2 = 0.07  validation_batch_size=BATCH_SIZE*4 \n","#rmsOpt = RMSprop(learning_rate = 0.003, weight_decay=0.00017) \n","\n","#BATCH_SIZE = 64 EPOCHS = 80 DROP_OUT1 = 0.0017 DROP_OUT2 = 0.01 LAMBDA1 = 0.017 LAMBDA2 = 0.09  validation_batch_size=BATCH_SIZE*4\n","#rmsOpt = RMSprop(learning_rate = 0.0035, weight_decay=0.00025) "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFT2AzS5/CVz3CktjX0DTN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
