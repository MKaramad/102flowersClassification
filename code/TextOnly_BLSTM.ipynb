{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2734,"status":"ok","timestamp":1721139598071,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"CXOP_6ongkzl"},"outputs":[],"source":["import os\n","import pandas as pd\n","import tensorflow.keras.backend as K\n","from gensim.corpora import Dictionary\n","from keras.preprocessing.sequence import pad_sequences\n","from hazm import Normalizer, POSTagger, word_tokenize, Lemmatizer, stopwords_list\n","from parsivar import FindStems\n","import numpy as np\n","from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D,Reshape, Flatten, Dropout, Concatenate\n","from keras.models import Model\n","from keras.layers import BatchNormalization \n","from keras import regularizers\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Bidirectional, Attention,  Embedding, LSTM, Dense\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1352,"status":"ok","timestamp":1721139601544,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"7FTtouNSgnYl"},"outputs":[],"source":["# Set Directories \n","\n","pardir = os.path.dirname(os.getcwd())\n","PATH = os.path.join(pardir,\"data\\\\caption_all_fa\\\\\")\n","os.chdir(PATH)\n","SAVED = os.path.join(pardir,\"data\\\\saved\\\\\")"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721139603451,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"46yq_155gpmS"},"outputs":[],"source":["# Prepare for data\n","\n","def load_doc(filename):\n","    file = open(file=filename, mode='r', encoding=\"utf-8\")\n","    text = file.read()\n","    file.close()\n","    return text\n","\n","# Map filename to image,text,label for train,evaluation and test\n","def load_clean_descriptions(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        if image_id in dataset:\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            desc = ' '.join(image_desc)\n","            descriptions[image_id].append(desc)\n","    return descriptions\n","\n","def load_clean_class(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","# OneHot\n","def load_class_dummy(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1]\n","        if image_desc in dataset:\n","            if image_desc not in descriptions:\n","                descriptions[image_desc] = list()\n","            image_id=dummies_dict[image_id]\n","            descriptions[image_desc].append(image_id)\n","    return descriptions\n","\n","### Class to one_hot_vector dictionary ###\n","folder_names = []\n","\n","for entry_name in os.listdir(PATH):\n","    entry_path = os.path.join(PATH, entry_name)\n","    if os.path.isdir(entry_path):\n","        folder_names.append(entry_name)\n","folder_names =sorted(folder_names)\n","\n","dummies = pd.get_dummies(folder_names)\n","dummies_list=dummies.values.tolist()\n","dummies_dict=dict(zip(folder_names,dummies_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9356,"status":"ok","timestamp":1721139616526,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"egoAXsijgyfB","outputId":"73f96de1-41e7-44c7-ebc9-cccf4d657d6d"},"outputs":[],"source":["### Text and label dictionary for train ###\n","filename = SAVED+'/train_image.txt'\n","train=[]\n","with open(file=filename, encoding='utf-8', mode='r') as f:\n","    for line in f.read().splitlines():\n","        train=train+[line.split(',')[0][:-4]]\n","\n","train_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","\n","train_class = load_clean_class(SAVED+'flower_class.txt', train)\n","print('Descriptions: train=%d' % len(train_class))\n","\n","class_dummy=load_class_dummy(SAVED+'flower_class.txt', train)\n","\n","### Text and label dictionary for EVAL ###\n","filename = SAVED+'/val_image.txt'\n","val=[]\n","with open(filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        val=val+[line.split(',')[0][:-4]]\n","\n","val_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', val)\n","print('Descriptions: val=%d' % len(val_descriptions))\n","\n","val_class = load_clean_class(SAVED+'flower_class.txt', val)\n","print('Descriptions: val=%d' % len(val_class))\n","val_class_dummy=load_class_dummy(SAVED+'flower_class.txt', val)\n","\n","\n","### Text and label dictionary for Test ###\n","filename = SAVED+'/test_image.txt'\n","test=[]\n","with open(file=filename, encoding='utf-8',mode='r') as f:\n","    for line in f.read().splitlines():\n","        test=test+[line.split(',')[0][:-4]]\n","\n","test_descriptions = load_clean_descriptions(SAVED+'flower_text_tagged_fa.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","\n","test_class = load_clean_class(SAVED+'flower_class.txt', test)\n","print('Descriptions: test=%d' % len(test_class))\n","test_class_dummy=load_class_dummy(SAVED+'flower_class.txt', test)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6932,"status":"ok","timestamp":1721139635361,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"i0QAqH3-g70J"},"outputs":[],"source":["# Load data\n","\n","punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*؟%؛،ًًًََُّ»«_~'''\n","def punc(myStr):\n","  no_punct = \"\"\n","  for char in myStr:\n","    if char not in punctuations:\n","        no_punct = no_punct + char\n","  return no_punct\n","\n","# Load morpheme dictionary for flowers\n","dictionary2=Dictionary.load(SAVED+\"dictionary/flower_dictionary2_fa_lemAndStemWithStopWords\")\n","\n","# Text loading and automatic preprocessing function\n","MAX_SEQUENCE_LENGTH=300\n","c=str()\n","\n","normalizer = Normalizer(correct_spacing=True)\n","lemmatizer = Lemmatizer(joined_verb_parts=False)\n","tagger = POSTagger(model=SAVED+'pos_tagger.model')\n","stop_words = stopwords_list()\n","my_stemmer = FindStems()\n","\n","def create_phrase(train_descriptions, class_dummy, MAX_SEQUENCE_LENGTH):\n","\n","    X_train, y_class = list(), list()\n","\n","    for key, desc_list in train_descriptions.items():\n","        c=list()\n","        for desc in desc_list:\n","\n","            seq = punc(desc)\n","            normalized_text = normalizer.normalize(seq)\n","            tokens = word_tokenize(normalized_text)\n","\n","            newTokens = []\n","\n","            tokenAndTag = tagger.tag(tokens)\n","            newTokenAndTag = []\n","            for TAT in tokenAndTag:\n","              newTokenAndTag.append([TAT[0], TAT[1].replace('PRON', 'PRO').replace('ADJ', 'AJ').replace('VERB', 'V').split(',')[0]])\n","            tokenAndTag = newTokenAndTag\n","\n","            j = 0\n","            for token in tokens :\n","              for lemToken in lemmatizer.lemmatize(word=token, pos=tokenAndTag[j][1]).split('#') :\n","                  if lemToken != '' :#if lemToken not in stop_words and lemToken != ''  :\n","                    newTokens.append(lemToken)\n","              for stemToken in my_stemmer.convert_to_stem(token).split('&') :\n","                  if  stemToken != ''  :#if stemToken not in stop_words and stemToken != ''  :\n","                    newTokens.append(stemToken)\n","              j += 1\n","            tokens = newTokens\n","            tokens = [token for token in tokens if token != '']\n","\n","            b = tagger.tag(tokens)\n","\n","            aaa=[]\n","            for y in b:\n","                aaa.append(y[0])\n","\n","            c+=aaa\n","        d=dictionary2.doc2idx(c)\n","        X_train.append(np.array(d))\n","        if key in class_dummy:\n","            y_class.append(np.array(class_dummy[key][0]))\n","\n","    X_train=pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n","    return np.array(X_train),np.array(y_class)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":120980,"status":"ok","timestamp":1721139758271,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"I3PNICWbhDns"},"outputs":[],"source":["# Import text into training, validation, and test sets\n","X_train, y_train = create_phrase(train_descriptions, class_dummy,MAX_SEQUENCE_LENGTH)\n","X_val, y_val = create_phrase(val_descriptions, val_class_dummy,MAX_SEQUENCE_LENGTH)\n","X_test, y_test = create_phrase(test_descriptions, test_class_dummy,MAX_SEQUENCE_LENGTH)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2595,"status":"ok","timestamp":1721139763327,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"YwCZxC4uhG_C","outputId":"dd33135b-a49e-4b9f-fbae-a7075b282006"},"outputs":[],"source":["# Load pre-trained embedding matrix\n","EMBEDDING_DIM=300\n","word_index=dictionary2.token2id\n","\n","embedding_path = SAVED + 'skipgram/skigram1_fa_new_300_lemStemWithStopWords.txt'\n","\n","embeddings_index = dict()\n","f = open(file = embedding_path, encoding= \"utf-8\")\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))\n","\n","embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"executionInfo":{"elapsed":5905,"status":"error","timestamp":1720948768373,"user":{"displayName":"mohammad karamad","userId":"09461906320193420501"},"user_tz":-210},"id":"-AcSrlIHa89a","outputId":"eef729ae-dbb3-484c-b34c-e955d2d476c0"},"outputs":[],"source":["### Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification - Zhou et al., 2016 ###\n","\n","def attention_lstm_model(lstm_units, DROP_OUT, LAMBDA1, LAMBDA2):\n","    inputs = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n","    embedding = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix], trainable=True)(inputs)\n","\n","    lstm_out = Bidirectional(LSTM(units=lstm_units, return_sequences=True))(embedding)\n","\n","    attention = Attention()([lstm_out, lstm_out])\n","\n","    attention = BatchNormalization()(attention)\n","\n","    attention = tf.reduce_mean(attention, axis=1)\n","\n","    dropout = Dropout(DROP_OUT)(attention)\n","    outputs = Dense(units=102, activation='softmax', kernel_regularizer=regularizers.L1L2(l1=LAMBDA1, l2=LAMBDA2), kernel_initializer='he_normal')(dropout)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model\n","\n","learning_rate = 0.002\n","lstm_units = 128\n","DROP_OUT = 0.002\n","BATCH_SIZE = 64\n","EPOCHS = 40\n","LAMBDA1 = 0.002\n","LAMBDA2 = 0.02\n","\n","\n","model = attention_lstm_model(lstm_units, DROP_OUT, LAMBDA1, LAMBDA2)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","#initial_learning_rate = 0.0025\n","#boundaries = [82, 246, 410, 820, 1230, 1640] #[1, 3, 5, 10, 15, 20]\n","#values = [0.002, 0.0015, 0.001, 0.0008, 0.0006, 0.0004, 0.0002]\n","#learning_rate_fn = PiecewiseConstantDecay(boundaries, values)\n","#rmsOpt = RMSprop(learning_rate = learning_rate_fn)\n","\n","rmsOpt = RMSprop(learning_rate=0.002)\n","model.compile(optimizer=rmsOpt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","#model.summary()\n","\n","history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val), callbacks=[early_stopping], validation_batch_size=BATCH_SIZE*4)\n","\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n","print(f'Test accuracy: {test_acc}')\n","#Test accuracy: 0.6393141746520996\n","#Test accuracy: 0.6576852202415466\n","#Test accuracy: 0.6870790123939514\n","#Test accuracy: 0.7293325066566467 lstm_units = 128 DROP_OUT = 0.00 BATCH_SIZE = 128 EPOCHS = 40 LAMBDA = 0.005\n","#Test accuracy: 0.6913655996322632 validation_batch_size=BATCH_SIZE*2 lstm_units = 64 DROP_OUT = 0.001\n","\n","#Test accuracy: 0.7397428154945374 lstm_units = 64 DROP_OUT = 0.001 BATCH_SIZE = 64 EPOCHS = 40 LAMBDA1 = 0.001 LAMBDA2 = 0.01 learning_rate = 0.002\n","#Test accuracy: 0.6497244238853455 lstm_units = 64 DROP_OUT = 0.01 BATCH_SIZE = 64 EPOCHS = 40 LAMBDA1 = 0.005 LAMBDA2 = 0.03 validation_batch_size=BATCH_SIZE*4\n","#Test accuracy: 0.6497244238853455 lstm_units = 64 DROP_OUT = 0.01 BATCH_SIZE = 64 EPOCHS = 40 LAMBDA1 = 0.005 LAMBDA2 = 0.03 validation_batch_size=BATCH_SIZE*4\n","#Test accuracy: 0.5903245806694031 DROP_OUT = 0.1 validation_batch_size=BATCH_SIZE*3 LAMBDA1 = 0.01 LAMBDA2 = 0.1 EPOCHS = 60 validation_batch_size=BATCH_SIZE*3\n","\n","\n","#Test accuracy: 0.7507654428482056 lstm_units = 128 DROP_OUT = 0.002 BATCH_SIZE = 64 EPOCHS = 40 LAMBDA1 = 0.002 LAMBDA2 = 0.02 learning_rate = 0.002 validation_batch_size=BATCH_SIZE*4\n","\n","#Test accuracy: 0.7452541589736938 lstm_units = 128 DROP_OUT = 0.004 BATCH_SIZE = 64 EPOCHS = 50 LAMBDA1 = 0.004 LAMBDA2 = 0.04\n","#learning_rate = 0.0025 validation_batch_size=BATCH_SIZE values = [0.0025, 0.002, 0.0017, 0.0015, 0.001, 0.0007, 0.0005] boundaries = [82, 246, 410, 820, 1230, 1640]\n","\n","#lstm_units = 128 DROP_OUT = 0.005 BATCH_SIZE = 64 EPOCHS = 70 LAMBDA1 = 0.005 LAMBDA2 = 0.05\n","#learning_rate = 0.0025 validation_batch_size=BATCH_SIZE values = [0.002, 0.0015, 0.001, 0.0008, 0.0006, 0.0004, 0.0002] boundaries = [82, 246, 410, 820, 1230, 1640]\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPFT2AzS5/CVz3CktjX0DTN","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}
